{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython import display\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "ROOT = '/work/dacb/what_is_a_hotdog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.RandomCrop(64, 64), transforms.Grayscale(1), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(ROOT + '/train', \n",
    "                        transform=trans,\n",
    "                        ),\n",
    "        batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(ROOT + '/test', \n",
    "                        transform=trans,\n",
    "                        ),\n",
    "        batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN(IMAGE_SIZE * IMAGE_SIZE, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    \n",
    "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "    \n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(x_data, y_data):\n",
    "    \n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu = torch.randn_like(net.out.weight)\n",
    "    outw_sigma = torch.randn_like(net.out.weight)\n",
    "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
    "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu = torch.randn_like(net.out.bias)\n",
    "    outb_sigma = torch.randn_like(net.out.bias)\n",
    "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
    "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss  19893.569856783677\n",
      "Epoch  1  Loss  18169.324914167635\n",
      "Epoch  2  Loss  16588.120469725116\n",
      "Epoch  3  Loss  15206.080131533156\n",
      "Epoch  4  Loss  13940.098629731307\n",
      "Epoch  5  Loss  12765.576792100741\n",
      "Epoch  6  Loss  11681.058192040122\n",
      "Epoch  7  Loss  10693.979579005367\n",
      "Epoch  8  Loss  9808.290670147206\n",
      "Epoch  9  Loss  9001.58061542535\n",
      "Epoch  10  Loss  8259.704119338894\n",
      "Epoch  11  Loss  7608.731888225417\n",
      "Epoch  12  Loss  6987.765127616365\n",
      "Epoch  13  Loss  6431.7895142245325\n",
      "Epoch  14  Loss  5931.080287154376\n",
      "Epoch  15  Loss  5456.412861542839\n",
      "Epoch  16  Loss  5059.492350975573\n",
      "Epoch  17  Loss  4657.11585958689\n",
      "Epoch  18  Loss  4321.989839046483\n",
      "Epoch  19  Loss  3984.887795882662\n",
      "Epoch  20  Loss  3689.988184086502\n",
      "Epoch  21  Loss  3420.5844165713456\n",
      "Epoch  22  Loss  3205.050836259179\n",
      "Epoch  23  Loss  3002.884604592844\n",
      "Epoch  24  Loss  2832.699727568154\n",
      "Epoch  25  Loss  2624.4453423726213\n",
      "Epoch  26  Loss  2475.139631533413\n",
      "Epoch  27  Loss  2321.470696641929\n",
      "Epoch  28  Loss  2176.7654958686685\n",
      "Epoch  29  Loss  2064.1623102532726\n",
      "Epoch  30  Loss  1976.060676190605\n",
      "Epoch  31  Loss  1898.6641305758335\n",
      "Epoch  32  Loss  1812.3560456523633\n",
      "Epoch  33  Loss  1710.9540704873157\n",
      "Epoch  34  Loss  1641.1446794129374\n",
      "Epoch  35  Loss  1540.6788335881538\n",
      "Epoch  36  Loss  1469.7375458629997\n",
      "Epoch  37  Loss  1411.7677042134285\n",
      "Epoch  38  Loss  1338.463822208055\n",
      "Epoch  39  Loss  1312.1400604451464\n",
      "Epoch  40  Loss  1251.8894531735812\n",
      "Epoch  41  Loss  1216.4221989872165\n",
      "Epoch  42  Loss  1180.5806389752415\n",
      "Epoch  43  Loss  1161.6921539833136\n",
      "Epoch  44  Loss  1150.5415691772996\n",
      "Epoch  45  Loss  1119.417739637224\n",
      "Epoch  46  Loss  1127.0784993093914\n",
      "Epoch  47  Loss  1116.2349780488344\n",
      "Epoch  48  Loss  1150.6128645493663\n",
      "Epoch  49  Loss  1081.210649514288\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 50\n",
    "loss = 0\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,IMAGE_SIZE*IMAGE_SIZE), data[1])\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    \n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network is forced to predict\n",
      "accuracy: 64 %\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    predicted = predict(images.view(-1,IMAGE_SIZE*IMAGE_SIZE))\n",
    "    total += labels.size(0)\n",
    "    correct += (torch.tensor(predicted) == labels).sum().item()\n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = dict([(value, key) for key, value in train_loader.dataset.class_to_idx.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotdog_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(ROOT + '/hotdog', \n",
    "                        transform=trans,\n",
    "                        ),\n",
    "        batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "\n",
    "predictions = {}\n",
    "for cls in idx_to_class.values():\n",
    "    predictions[cls] = 0\n",
    "    \n",
    "for j, data in enumerate(hotdog_loader):\n",
    "    images, labels = data\n",
    "    predicted = predict(images.view(-1,IMAGE_SIZE*IMAGE_SIZE))\n",
    "    for pred in predicted:\n",
    "        predictions[idx_to_class[pred]] = predictions[idx_to_class[pred]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(predictions.keys()), predictions.values(), color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burrito': 11,\n",
       " 'calzone': 10,\n",
       " 'lasagna': 18,\n",
       " 'muffins': 16,\n",
       " 'pizza': 6,\n",
       " 'salad': 7,\n",
       " 'sandwich': 20,\n",
       " 'soup': 1,\n",
       " 'sushi': 2}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotdog",
   "language": "python",
   "name": "hotdog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
